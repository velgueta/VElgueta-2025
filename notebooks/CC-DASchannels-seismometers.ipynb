{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime, read\n",
    "from scipy.signal import correlate, resample, butter, filtfilt\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "import h5py\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from obspy import Stream, UTCDateTime\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.signal import butter, filtfilt, tukey\n",
    "from scipy.signal import fftconvolve\n",
    "from matplotlib import cm\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fdea9",
   "metadata": {},
   "source": [
    "# Loading quakes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"IRIS\")\n",
    "network = 'CC'\n",
    "station = 'PARA'\n",
    "channel = 'BHZ' \n",
    "location = '*'\n",
    "event_time = UTCDateTime(2023, 8, 23, 10, 0, 0)\n",
    "event_endtime = UTCDateTime(2023, 11, 23, 10, 0, 0)\n",
    "#event_endtime = UTCDateTime(2023, 9, 23, 10, 0, 0)\n",
    "#duration = 60 * 7200 #120  # Duración de una dos en segundos\n",
    "\n",
    "# References\n",
    "latitude = 46.87  # Ejemplo de latitud\n",
    "longitude = -121.760  # Ejemplo de longitud\n",
    "max_radius_km = 50  # Radio máximo en km, before was 250km\n",
    "\n",
    "# Convertir el radio máximo de kilómetros a grados\n",
    "max_radius_deg = max_radius_km / 111.19\n",
    "\n",
    "# Buscar todos los terremotos que ocurrieron en el rango de una hora y dentro del radio especificado\n",
    "catalog = client.get_events(\n",
    "    starttime=event_time,\n",
    "    endtime=event_endtime,\n",
    "    #minmagnitude=2.5,\n",
    "    minmagnitude=0,#antes 1.8\n",
    "    latitude=latitude,\n",
    "    longitude=longitude,\n",
    "    maxradius=max_radius_deg\n",
    ")\n",
    "#catalog to df\n",
    "data = []\n",
    "for event in catalog:\n",
    "    origin = event.origins[0]\n",
    "    magnitude = event.magnitudes[0]\n",
    "    eventid_full= event.resource_id.id if event.resource_id else None\n",
    "    event_id = eventid_full.split('eventid=')[-1] if eventid_full else None\n",
    "\n",
    "    \n",
    "    data.append({\n",
    "        'getstring': origin.time,\n",
    "        'Latitude': origin.latitude,\n",
    "        'Longitude': origin.longitude,\n",
    "        'Depth (km)': origin.depth / 1000,  # convertir de metros a kilómetros\n",
    "        'Magnitude': magnitude.mag,\n",
    "        'Magnitude Type': magnitude.magnitude_type,\n",
    "        'evid':event_id\n",
    "        \n",
    "    })\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el tipo actual de 'getstring'\n",
    "print(df['getstring'].dtype) #object\n",
    "\n",
    "# Convertir la columna 'getstring' directamente a UTCDateTime\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        event_time = row['getstring']\n",
    "        \n",
    "        if pd.isna(event_time):\n",
    "            print(f\"Skipping event {row['evid']} due to NaT in getstring\")\n",
    "            continue\n",
    "\n",
    "        # Convertir a UTCDateTime\n",
    "        event_time = UTCDateTime(event_time)\n",
    "        #print(f\"Processing event {row['evid']} with time {event_time}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing event {row['evid']}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3adcfd5",
   "metadata": {},
   "source": [
    "# Loading DAS data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3255abb",
   "metadata": {},
   "source": [
    "fuctions for cross-correlation and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_das_data(file_paths, start_time, duration_seconds, chan_min, chan_max):\n",
    "    \"\"\"\n",
    "    Load and concatenate DAS segments from HDF5 files whose 60 s windows\n",
    "    overlap [start_time, start_time+duration_seconds].\n",
    "    Automatically discovers the first dataset name via visititems().\n",
    "    \"\"\"\n",
    "    # 1) Discover the internal dataset name\n",
    "    dataset_name = None\n",
    "    class _Found(Exception): pass\n",
    "\n",
    "    for path in sorted(file_paths):\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            def _visitor(name, obj):\n",
    "                nonlocal dataset_name\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    dataset_name = name\n",
    "                    raise _Found()\n",
    "            try:\n",
    "                f.visititems(_visitor)\n",
    "            except _Found:\n",
    "                pass\n",
    "        if dataset_name:\n",
    "            break\n",
    "\n",
    "    if not dataset_name:\n",
    "        raise ValueError(\"No dataset found in any DAS HDF5 file.\")\n",
    "\n",
    "    # 2) Identify candidate files overlapping our window\n",
    "    end_time = start_time + duration_seconds\n",
    "    pat = re.compile(r'decimator_(\\d{4}-\\d{2}-\\d{2})_(\\d{2}\\.\\d{2}\\.\\d{2})_UTC\\.h5$')\n",
    "    candidates = []\n",
    "    for path in file_paths:\n",
    "        m = pat.search(path)\n",
    "        if not m:\n",
    "            continue\n",
    "        date_str, time_str = m.groups()\n",
    "        ts = UTCDateTime(f\"{date_str}T{time_str.replace('.', ':')}\")\n",
    "        if ts < end_time and (ts + 60) > start_time:\n",
    "            candidates.append((ts, path))\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No DAS files overlap the requested time window.\")\n",
    "\n",
    "    # 3) Load, offset, and concatenate until we have enough samples\n",
    "    chunks       = []\n",
    "    used_files   = []\n",
    "    fs           = None\n",
    "    total_needed = None\n",
    "    collected    = 0\n",
    "\n",
    "    for file_start, path in candidates:\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            arr = f[dataset_name][:, chan_min:chan_max+1]\n",
    "\n",
    "        if fs is None:\n",
    "            fs = arr.shape[0] / 60.0\n",
    "            total_needed = int(round(duration_seconds * fs))\n",
    "\n",
    "        offset = max(0, int(round((start_time - file_start) * fs)))\n",
    "        chunk  = arr[offset:]\n",
    "        take   = min(chunk.shape[0], total_needed - collected)\n",
    "        if take > 0:\n",
    "            chunks.append(chunk[:take])\n",
    "            used_files.append(path)\n",
    "            collected += take\n",
    "        if collected >= total_needed:\n",
    "            break\n",
    "\n",
    "    if collected < total_needed:\n",
    "        print(f\"Warning: only collected {collected}/{total_needed} samples.\")\n",
    "\n",
    "    data_concat = np.vstack(chunks)[:total_needed]\n",
    "    return data_concat, fs, used_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e9ceb",
   "metadata": {},
   "source": [
    "#  Events cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seismic_trace(network: str,\n",
    "                      station: str,\n",
    "                      location: str,\n",
    "                      channel: str,\n",
    "                      start_time: UTCDateTime,\n",
    "                      duration_seconds: float,\n",
    "                      freqmin: float = 2.0,\n",
    "                      freqmax: float = 10.0):\n",
    "    \"\"\"\n",
    "    Download and pre‐process a single seismometer trace.\n",
    "    - Removes instrument response (if RESP files are available in your ObsPy config).\n",
    "    - Detrends, demeans, band‐pass filters, and applies a taper.\n",
    "    \"\"\"\n",
    "    client = Client(\"IRIS\")\n",
    "    end_time = start_time + duration_seconds\n",
    "\n",
    "    # 1. Fetch raw data\n",
    "    st = client.get_waveforms(network, station, location, channel,\n",
    "                              start_time, end_time, attach_response=True)\n",
    "\n",
    "    # 2. Remove instrument response to get true ground velocity\n",
    "    st.remove_response(output=\"VEL\", zero_mean=True, taper=\"hann\")\n",
    "\n",
    "    # 3. Detrend & demean\n",
    "    st.detrend('linear')\n",
    "    st.detrend('demean')\n",
    "\n",
    "    # 4. Bandpass between freqmin–freqmax\n",
    "    st.filter('bandpass', freqmin=freqmin, freqmax=freqmax, corners=4, zerophase=True)\n",
    "\n",
    "    # 5. Taper edges with a Tukey window (alpha=0.1)\n",
    "    for tr in st:\n",
    "        tr.data *= tukey(tr.stats.npts, alpha=0.1)\n",
    "\n",
    "    # We assume a single trace in the stream\n",
    "    return st[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lag_times(\n",
    "    seismic_trace,\n",
    "    das_data: np.ndarray,\n",
    "    fs: float,\n",
    "    corr_threshold: float = 0.8,\n",
    "    lowcut: float = 2.0,\n",
    "    highcut: float = 10.0\n",
    "):\n",
    "    \"\"\"\n",
    "    cross-correlate each das channel with the seismic trace.\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    median_lag : float | None   # median of lag times for channels passing the threshold\n",
    "    lag_std    : float | None   # standard deviation of those lags\n",
    "    n_channels : int            # number of channels that exceeded the threshold\n",
    "    peak_corr  : float          # global correlation peak (max abs across all channels)\n",
    "    \"\"\"\n",
    "    # 1) band-pass filter das data\n",
    "    b, a = butter(4, [lowcut, highcut], btype='bandpass', fs=fs)\n",
    "    das_filtered = filtfilt(b, a, das_data, axis=0)\n",
    "\n",
    "    # 2) taper window\n",
    "    n_samples, n_ch = das_filtered.shape\n",
    "    window = tukey(n_samples, alpha=0.1)\n",
    "\n",
    "    # 3) demean, detrend & taper\n",
    "    for ch_idx in range(n_ch):\n",
    "        ch = das_filtered[:, ch_idx]\n",
    "        ch = ch - np.mean(ch)\n",
    "        trend = np.poly1d(np.polyfit(np.arange(n_samples), ch, 1))\n",
    "        das_filtered[:, ch_idx] = (ch - trend(np.arange(n_samples))) * window\n",
    "\n",
    "    # 4) normalize seismic trace\n",
    "    seismic = seismic_trace.data.astype(float)\n",
    "    seismic -= np.mean(seismic)\n",
    "    seismic /= max(np.linalg.norm(seismic), 1e-12)\n",
    "\n",
    "    # 5) cross-correlate\n",
    "    zero_idx = len(seismic) - 1\n",
    "    lags, accepted_corrs = [], []\n",
    "    peak_corr = 0.0  # value to report\n",
    "\n",
    "    for ch_idx in range(n_ch):\n",
    "        chan = das_filtered[:, ch_idx]\n",
    "        norm = np.linalg.norm(chan)\n",
    "        if norm < 1e-12:\n",
    "            continue\n",
    "        chan /= norm\n",
    "\n",
    "        corr = fftconvolve(seismic, chan[::-1], mode='full')\n",
    "        pk   = float(np.max(np.abs(corr)))  # cast from float32 to float\n",
    "        peak_corr = max(peak_corr, pk)      # update global peak\n",
    "\n",
    "        if pk < corr_threshold:\n",
    "            continue\n",
    "\n",
    "        lag_sec = (np.argmax(np.abs(corr)) - zero_idx) / fs\n",
    "        lags.append(lag_sec)\n",
    "        accepted_corrs.append(pk)\n",
    "\n",
    "    # 6) statistics\n",
    "    if not lags:\n",
    "        return None, None, 0, peak_corr\n",
    "\n",
    "    median_lag = float(np.median(lags))\n",
    "    lag_std    = float(np.std(lags))\n",
    "    return median_lag, lag_std, len(lags), peak_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seismic_and_das_with_offset(\n",
    "    time_axis: np.ndarray,\n",
    "    seismic_trace,\n",
    "    filtered_das_data: np.ndarray,\n",
    "    event_id: str,\n",
    "    origin_time,\n",
    "    magnitude: float,\n",
    "    best_corr_lag: float,\n",
    "    seismic_scale: float = 3.0,\n",
    "    das_scaling_factor: float = 5.0,\n",
    "    channel_offset_step: float = 1.5,\n",
    "    colormap: str = 'viridis'\n",
    "):\n",
    "\n",
    "    # 1. Normalize seismic trace by its maximum absolute value\n",
    "    data = seismic_trace.data.astype(float)\n",
    "    max_seis = np.max(np.abs(data))\n",
    "    if max_seis > 0:\n",
    "        seis_norm = (data / max_seis) * seismic_scale\n",
    "    else:\n",
    "        seis_norm = data\n",
    "\n",
    "    # 2. Normalize DAS data by global max and apply scaling\n",
    "    global_max = np.max(np.abs(filtered_das_data))\n",
    "    if global_max > 0:\n",
    "        das_norm = (filtered_das_data / global_max) * das_scaling_factor\n",
    "    else:\n",
    "        das_norm = filtered_das_data.copy()\n",
    "\n",
    "    n_samples, n_ch = das_norm.shape\n",
    "    offsets = np.arange(n_ch) * channel_offset_step\n",
    "\n",
    "    # 3. Prepare color map\n",
    "    cmap = cm.get_cmap(colormap, n_ch)\n",
    "    colors = cmap(np.linspace(0, 1, n_ch))\n",
    "\n",
    "    # 4. Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot seismic trace\n",
    "    ax.plot(time_axis, seis_norm, 'k', linewidth=1.5, label='Seismic Trace')\n",
    "\n",
    "    # Plot each DAS channel with its offset and color\n",
    "    for i in range(n_ch):\n",
    "        ax.plot(time_axis, das_norm[:, i] + offsets[i],\n",
    "                color=colors[i], linewidth=0.8)\n",
    "    # Only include one legend entry for DAS\n",
    "    ax.plot([], [], color='gray', linewidth=0.8, label='DAS Channels')\n",
    "\n",
    "    # 5. Annotate best‐lag with vertical line\n",
    "    ax.axvline(best_corr_lag, color='red', linestyle='--', linewidth=1)\n",
    "    ax.text(best_corr_lag + 0.02,  # small horizontal shift\n",
    "            offsets[-1] + das_scaling_factor * 0.2,\n",
    "            f'Lag = {best_corr_lag:.3f} s',\n",
    "            color='red', fontsize=9, va='bottom')\n",
    "\n",
    "    # 6. Optionally label a few channel offsets on y‑axis\n",
    "    #    (here we show every 10th channel)\n",
    "    tick_idxs = np.arange(0, n_ch, max(1, n_ch // 10))\n",
    "    tick_locs = offsets[tick_idxs]\n",
    "    tick_labels = [f'Ch {i}' for i in tick_idxs]\n",
    "    ax.set_yticks(list(tick_locs) + [0])  # include zero for seismic\n",
    "    ax.set_yticklabels(tick_labels + ['Seis'])\n",
    "\n",
    "    # 7. Labels, title, legend and grid\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Normalized Amplitude + Offset\")\n",
    "    ax.set_title(\n",
    "        f\"Event {event_id} | {origin_time.strftime('%Y-%m-%d %H:%M:%S UTC')}\\n\"\n",
    "        f\"Magnitude {magnitude:.2f} | Best Corr Lag: {best_corr_lag:.3f} s\"\n",
    "    )\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a86fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift_array(data: np.ndarray, shift_samples: int) -> np.ndarray:\n",
    "    shifted = np.zeros_like(data)\n",
    "    if shift_samples > 0:\n",
    "        shifted[shift_samples:] = data[:-shift_samples]\n",
    "    elif shift_samples < 0:\n",
    "        shifted[:shift_samples] = data[-shift_samples:]\n",
    "    else:\n",
    "        shifted[:] = data\n",
    "    return shifted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User parameters\n",
    "# ----------------------------------------\n",
    "threshold = 0.34\n",
    "das_root = '/1-fnp/petasaur/p-jbod1/rainier/2023'\n",
    "file_paths = glob(os.path.join(das_root, '**', '*.h5'), recursive=True)\n",
    "\n",
    "network, station, location, channel = \"CC\", \"PARA\", \"*\", \"BHZ\"\n",
    "#network, station, location, channel = \"UW\", \"LON\", \"*\", \"HHZ\"\n",
    "\n",
    "chan_min, chan_max = 60, 90\n",
    "#chan_min, chan_max = 2000, 2030\n",
    "\n",
    "duration_seconds = 20\n",
    "\n",
    "output_csv = f'./results_lag_times_{station}_{threshold}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder to save PNG plots\n",
    "output_plot_dir = \"/data/data4/veronica-scratch-rainier/shift-time/\"\n",
    "os.makedirs(output_plot_dir, exist_ok=True)\n",
    "\n",
    "df = df.dropna(subset=['getstring'])\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    ev_id = row['evid']\n",
    "    t_str = row['getstring']\n",
    "    mag = row['Magnitude']\n",
    "    \n",
    "    if pd.isna(t_str):\n",
    "        print(f\"Skipping event {ev_id} due to NaT in getstring\")\n",
    "        results.append({\n",
    "            'Magnitude': mag,\n",
    "            'event_id': ev_id,\n",
    "            'event_time': None,\n",
    "            'median_lag': None,\n",
    "            'lag_std': None,\n",
    "            'n_channels': None,\n",
    "            'das_files': None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        event_time = UTCDateTime(t_str)\n",
    "        start = event_time - 5\n",
    "        seismic = get_seismic_trace(\n",
    "            network, station, location, channel,\n",
    "            start, duration_seconds\n",
    "        )\n",
    "\n",
    "        das_data, fs, used_files = load_das_data(\n",
    "            file_paths, start, duration_seconds,\n",
    "            chan_min, chan_max\n",
    "        )\n",
    "        if not used_files:\n",
    "            print(f\"No DAS data for event {ev_id}; skipping.\")\n",
    "            results.append({\n",
    "                'Magnitude': mag,\n",
    "                'event_id': ev_id,\n",
    "                'event_time': event_time.isoformat(),\n",
    "                'median_lag': None,\n",
    "                'lag_std': None,\n",
    "                'n_channels': None,\n",
    "                'das_files': None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        das_resampled = resample(das_data, len(seismic.data), axis=0)\n",
    "\n",
    "        median_lag, lag_std, nch, peak_corr = calculate_lag_times(\n",
    "            seismic, das_resampled,\n",
    "            seismic.stats.sampling_rate,\n",
    "            corr_threshold=threshold\n",
    "        )\n",
    "        if median_lag is None:\n",
    "            print(f\"No channels passed threshold for event {ev_id}; skipping.\")\n",
    "            results.append({\n",
    "                'Magnitude': mag,\n",
    "                'event_id': ev_id,\n",
    "                'event_time': event_time.isoformat(),\n",
    "                'median_lag': None,\n",
    "                'lag_std': None,\n",
    "                'n_channels': 0,\n",
    "                'peak': peak_corr,\n",
    "                'das_files': ','.join(os.path.basename(p) for p in used_files)\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Registro exitoso\n",
    "        results.append({\n",
    "            'Magnitude': mag,\n",
    "            'event_id': ev_id,\n",
    "            'event_time': event_time.isoformat(),\n",
    "            'median_lag': median_lag,\n",
    "            'lag_std': lag_std,\n",
    "            'n_channels': nch,\n",
    "            'peak': peak_corr,\n",
    "            'das_files': ','.join(os.path.basename(p) for p in used_files)\n",
    "        })\n",
    "\n",
    "        # Plotting y guardado\n",
    "        time_axis = np.linspace(0, duration_seconds, len(seismic.data))\n",
    "        seis_norm = seismic.data / np.max(np.abs(seismic.data))\n",
    "        shift_samps = int(round(median_lag * seismic.stats.sampling_rate))\n",
    "        corrected = time_shift_array(das_resampled, shift_samps)\n",
    "\n",
    "        def normalize(arr):\n",
    "            epsilon = 1e-10\n",
    "            mx = np.max(np.abs(arr))\n",
    "            return (arr / mx) * 2 if mx > epsilon else arr\n",
    "\n",
    "        das_before = normalize(das_resampled)\n",
    "        das_after = normalize(corrected)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "        for ax, data, label in zip([ax1, ax2], [das_before, das_after], ['Before', 'After']):\n",
    "            ax.plot(time_axis, 3 * seis_norm, 'k', label='Seismic Trace')\n",
    "            offset = 4\n",
    "            for ch in data.T:\n",
    "                ax.plot(time_axis, 10 * ch + offset, color='teal')\n",
    "                offset += 1.5\n",
    "            ax.set_ylabel(\"Normalized Amplitude\")\n",
    "            ax.set_title(\n",
    "                f\"{label} Correction | Event {ev_id} | \"\n",
    "                f\"Lag = {median_lag:.3f}s | Magnitude = {mag}\"\n",
    "            )\n",
    "            ax.grid(True)\n",
    "        ax2.set_xlabel(\"Time (s)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_plot_dir, f'correction_{ev_id}.png'), dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Event {ev_id} error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # También lo registramos como fallo en el CSV\n",
    "        results.append({\n",
    "            'Magnitude': mag,\n",
    "            'event_id': ev_id,\n",
    "            'event_time': t_str,\n",
    "            'median_lag': None,\n",
    "            'lag_std': None,\n",
    "            'n_channels': None,\n",
    "            'das_files': None,\n",
    "            'peak': peak_corr\n",
    "        })\n",
    "\n",
    "# 6. Save all results to CSV\n",
    "pd.DataFrame(results).to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a15ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build DataFrame and drop rows with no event_time at all\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.dropna(subset=['event_time'])\n",
    "\n",
    "# 2. Convert ObsPy UTCDateTime → string → pandas datetime\n",
    "results_df['event_time'] = pd.to_datetime(\n",
    "    results_df['event_time'].astype(str),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# (Optional) Drop any rows that still failed parsing\n",
    "results_df = results_df.dropna(subset=['event_time'])\n",
    "\n",
    "# 3. Filter only valid lag values in (0,4]\n",
    "# 3. Save the complete results (before filtering)\n",
    "complete_csv = os.path.join(output_plot_dir, \"all-quakes-3months-nofilter-034.csv\")\n",
    "results_df.to_csv(complete_csv, index=False)\n",
    "print(f\"\\nComplete results saved to {complete_csv}\")\n",
    "\n",
    "# 4. Apply the lag filter and save filtered CSV\n",
    "filtered_results = results_df[\n",
    "    results_df['median_lag'].notna() &\n",
    "    (results_df['median_lag'] > 0.0) &\n",
    "    (results_df['median_lag'] <= 12)\n",
    "]\n",
    "\n",
    "# 4. Compute average and save\n",
    "average_lag_time = filtered_results['median_lag'].mean()\n",
    "print(f\"Average Lag Time (filtered): {average_lag_time:.4f} s\")\n",
    "\n",
    "filtered_results.to_csv(output_csv, index=False)\n",
    "print(f\"\\nFiltered results saved to {output_csv}\")\n",
    "\n",
    "# 5. Scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(\n",
    "    filtered_results['event_time'],\n",
    "    filtered_results['median_lag'],\n",
    "    s=150, marker='o', edgecolor='k' \n",
    ")\n",
    "#ax.axhline(\n",
    "#    average_lag_time, linestyle='--', linewidth=1.5,\n",
    "#    label=f'Avg: {average_lag_time:.2f} s'\n",
    "#)\n",
    "ax.set_ylim(0, filtered_results['median_lag'].max() * 1.1)\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha='right', fontsize=18)\n",
    "ax.set_xlabel('Event Time', fontsize=16)\n",
    "ax.set_ylabel('Lag Time (s)', fontsize=16)\n",
    "#ax.set_title(f'Lag Times for Station {station} (thr={threshold}), window=20s', fontsize=14)\n",
    "ax.grid(True, linestyle=':', alpha=0.7)\n",
    "ax.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Lagtime-allmagnitude', dpi = 300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Obspy+LibComtCat)",
   "language": "python",
   "name": "python-obspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
