{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2cbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import obspy\n",
    "import glob \n",
    "import matplotlib.dates as mdates \n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "import h5py\n",
    "import matplotlib.ticker as mticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276403d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/home/velgueta/notebooks/RainierDas/Notebooks/Phase-picking/csv/MORADAS_20230827_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ccf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the desired columns\n",
    "selected_df = df[['station_code', 'trace_s_arrival', 'trace_p_arrival']]\n",
    "\n",
    "# Drop rows that have NaN in both 'trace_s_arrival' and 'trace_p_arrival' columns\n",
    "cleaned_df = selected_df.dropna(subset=['trace_s_arrival', 'trace_p_arrival'], how='all')\n",
    "\n",
    "# Remove the word \"das\" from the 'station_code' column\n",
    "cleaned_df['station_code'] = cleaned_df['station_code'].str.replace('das', '')\n",
    "\n",
    "# Convert the time columns to datetime type\n",
    "cleaned_df['trace_s_arrival'] = pd.to_datetime(cleaned_df['trace_s_arrival'], errors='coerce')\n",
    "cleaned_df['trace_p_arrival'] = pd.to_datetime(cleaned_df['trace_p_arrival'], errors='coerce')\n",
    "\n",
    "# Extract the minutes from both time columns and take the minimum of both for each row\n",
    "cleaned_df['minute'] = cleaned_df[['trace_s_arrival', 'trace_p_arrival']].min(axis=1).dt.floor('T')\n",
    "\n",
    "# Group the rows by the value of minutes and save each group into a separate DataFrame\n",
    "grouped_dfs = {minute: group.drop(columns='minute') for minute, group in cleaned_df.groupby('minute')}\n",
    "\n",
    "# Save each DataFrame to a separate CSV file\n",
    "for minute, df_group in grouped_dfs.items():\n",
    "    minute_str = minute.strftime('%Y-%m-%d_%H-%M')\n",
    "    df_group.to_csv(f'pickerbyhour_2023-08-27_10/file_{minute_str}.csv', index=False)\n",
    "\n",
    "# Display an example of the separated DataFrames\n",
    "for minute, df_group in grouped_dfs.items():\n",
    "    print(f\"Minute: {minute}\")\n",
    "    print(df_group.head())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccf695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chan_min=0\n",
    "chan_max=-1\n",
    "\n",
    "#h5 file\n",
    "data_file = h5py.File('/1-fnp/petasaur/p-wd07/rainier/decimator_2023-12-10_12.19.00_UTC.h5','r')\n",
    "\n",
    "\n",
    "this_data = np.array(data_file['Acquisition/Raw[0]/RawData'][:,chan_min:chan_max])\n",
    "this_time = np.array(data_file['Acquisition/Raw[0]/RawDataTime'])\n",
    "            \n",
    "attrs=dict(data_file['Acquisition'].attrs)\n",
    "\n",
    "data_file.close()\n",
    "                            \n",
    "channel_number = chan_max -chan_min\n",
    "low_cut1 = 2.5\n",
    "hi_cut1 = 10\n",
    "fs=attrs['MaximumFrequency']*2\n",
    "print(fs)\n",
    "\n",
    "#filter\n",
    "\n",
    "b,a = butter(2,(low_cut1,hi_cut1),'bp',fs=fs)\n",
    "data_filt = filtfilt(b,a,this_data,axis=0)\n",
    "date_format = mdates.DateFormatter('%H:%M:%S')\n",
    "x_lims = mdates.date2num(this_time)\n",
    "#x_max = data_filt.shape[1] * attrs['SpatialSamplingInterval']\n",
    "#dx = x_max / data_filt.shape[1]\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(15,10))\n",
    "\n",
    "plt.imshow(data_filt.T,cmap='seismic',aspect='auto',vmin=-0.05,vmax=0.05,extent=[x_lims[0],x_lims[-1],data_filt.shape[1],0])\n",
    "plt.xlabel(\"Time UTC\", fontsize=25)\n",
    "plt.ylabel(\" DAS channels \", fontsize=25)\n",
    "\n",
    "\n",
    "ax.xaxis.set_major_formatter(date_format)\n",
    "ax.xaxis_date()\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "#here is where we insert the point from the csv\n",
    "picks = 'pickerbyhour_2023-12-10_12/file_2023-12-10_12-19.csv'\n",
    "selected_df = pd.read_csv(picks)\n",
    "\n",
    "p_wave_label_added = False\n",
    "s_wave_label_added = False\n",
    "\n",
    "for index, row in selected_df.iterrows():\n",
    "    blue_point_time = row['trace_s_arrival']\n",
    "    print(blue_point_time)\n",
    "    red_point_time = row['trace_p_arrival']\n",
    "    print(red_point_time)\n",
    "    station_code = int(row['station_code'])\n",
    "\n",
    "    if pd.notna(red_point_time):\n",
    "        red_point_date = mdates.date2num([red_point_time])[0]\n",
    "        ax.plot(red_point_date, station_code, 'ro', markersize=3, label='P-wave' if not p_wave_label_added else \"\")\n",
    "        p_wave_label_added = True\n",
    "\n",
    "    if pd.notna(blue_point_time):\n",
    "        blue_point_date = mdates.date2num([blue_point_time])[0]\n",
    "        ax.plot(blue_point_date, station_code, 'bo', markersize=3, label='S-wave' if not s_wave_label_added else \"\")\n",
    "        s_wave_label_added = True\n",
    "        \n",
    "ax.legend(fontsize=20)\n",
    "plt.savefig('./picks_2023-12-10-12-19.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ae24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "df = pd.read_csv('/home/velgueta/notebooks/RainierDas/Notebooks/Phase-picking/MORADAS_avalanche.csv')\n",
    "\n",
    "# Seleccionar las columnas deseadas\n",
    "selected_df = df[['station_code', 'trace_s_arrival', 'trace_p_arrival']]\n",
    "\n",
    "# Eliminar filas que tienen NaN en ambas columnas 'trace_s_arrival' y 'trace_p_arrival'\n",
    "cleaned_df = selected_df.dropna(subset=['trace_s_arrival', 'trace_p_arrival'], how='all')\n",
    "\n",
    "# Remover la palabra \"das\" de la columna 'station_code'\n",
    "cleaned_df['station_code'] = cleaned_df['station_code'].str.replace('das', '')\n",
    "\n",
    "# Convertir las columnas de tiempo a tipo datetime\n",
    "cleaned_df['trace_s_arrival'] = pd.to_datetime(cleaned_df['trace_s_arrival'], errors='coerce')\n",
    "cleaned_df['trace_p_arrival'] = pd.to_datetime(cleaned_df['trace_p_arrival'], errors='coerce')\n",
    "\n",
    "# Extraer los minutos de ambas columnas de tiempo y tomar el mínimo de ambos para cada fila\n",
    "cleaned_df['minute'] = cleaned_df[['trace_s_arrival', 'trace_p_arrival']].min(axis=1).dt.floor('T')\n",
    "\n",
    "# Guardar el DataFrame limpio en un nuevo archivo CSV\n",
    "cleaned_df.to_csv('/home/velgueta/notebooks/RainierDas/Notebooks/Phase-picking/MORADAS_avalanche_cleaned.csv', index=False)\n",
    "\n",
    "# Mostrar una vista previa del DataFrame limpio\n",
    "#print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdef18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chan_min = 0\n",
    "chan_max = -1\n",
    "\n",
    "# h5 file\n",
    "data_file = h5py.File('/home/velgueta/notebooks/RainierDas/Notebooks/Phase-picking/decimator_2023-12-10_12.18.00_UTC.h5', 'r')\n",
    "\n",
    "this_data = np.array(data_file['Acquisition/Raw[0]/RawData'][:, chan_min:chan_max])\n",
    "this_time = np.array(data_file['Acquisition/Raw[0]/RawDataTime'])\n",
    "\n",
    "attrs = dict(data_file['Acquisition'].attrs)\n",
    "\n",
    "data_file.close()\n",
    "\n",
    "channel_number = chan_max - chan_min\n",
    "low_cut1 = 2\n",
    "hi_cut1 = 10\n",
    "fs = attrs['MaximumFrequency'] * 2\n",
    "print(fs)\n",
    "\n",
    "# Filter\n",
    "b, a = butter(2, (low_cut1, hi_cut1), 'bp', fs=fs)\n",
    "data_filt = filtfilt(b, a, this_data, axis=0)\n",
    "date_format = mdates.DateFormatter('%H:%M:%S')\n",
    "x_lims = mdates.date2num(this_time)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "plt.imshow(data_filt.T, cmap='seismic', aspect='auto', vmin=-0.05, vmax=0.05, extent=[x_lims[0], x_lims[-1], data_filt.shape[1], 0])\n",
    "plt.xlabel(\"Time UTC\", fontsize=20)\n",
    "plt.ylabel(\"Optical distance (km)\", fontsize=20)\n",
    "\n",
    "ax.xaxis.set_major_formatter(date_format)\n",
    "ax.xaxis_date()\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "# Insert the point from the CSV\n",
    "picks = '/home/velgueta/notebooks/RainierDas/Notebooks/Phase-picking/MORADAS_avalanche_cleaned.csv'\n",
    "selected_df = pd.read_csv(picks)\n",
    "\n",
    "p_wave_label_added = False\n",
    "s_wave_label_added = False\n",
    "\n",
    "for index, row in selected_df.iterrows():\n",
    "    blue_point_time = row['trace_s_arrival']\n",
    "    print(blue_point_time)\n",
    "    red_point_time = row['trace_p_arrival']\n",
    "    print(red_point_time)\n",
    "    \n",
    "    station_code = row['station_code']\n",
    "    \n",
    "    try:\n",
    "        station_code = int(station_code)  # Convert to integer if possible\n",
    "    except ValueError:\n",
    "        print(f\"Non-numeric station_code encountered: {station_code}\")\n",
    "        continue  # Skip this row if station_code is non-numeric\n",
    "    \n",
    "    if pd.notna(red_point_time):\n",
    "        red_point_date = mdates.date2num([red_point_time])[0]\n",
    "        ax.plot(red_point_date, station_code, 'ro', markersize=1, label='P-wave' if not p_wave_label_added else \"\")\n",
    "        p_wave_label_added = True\n",
    "\n",
    "    if pd.notna(blue_point_time):\n",
    "        blue_point_date = mdates.date2num([blue_point_time])[0]\n",
    "        ax.plot(blue_point_date, station_code, 'bo', markersize=1, label='S-wave' if not s_wave_label_added else \"\")\n",
    "        s_wave_label_added = True\n",
    "\n",
    "ax.legend(fontsize=20)\n",
    "plt.savefig('./picks_2023-12-10-12-19.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91418441",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_min =  0\n",
    "chan_max = -1\n",
    "low_cut1 =  2\n",
    "hi_cut1  =  9\n",
    "\n",
    "    \n",
    "def plot_data_first(ax, file_path, zoom_start_str, zoom_end_str, picks_path,label):\n",
    "    # Cargar los datos\n",
    "    data_file = h5py.File(file_path, 'r')\n",
    "    this_data = np.array(data_file['Acquisition/Raw[0]/RawData'][:, chan_min:chan_max])\n",
    "    this_time = np.array(data_file['Acquisition/Raw[0]/RawDataTime'])\n",
    "    attrs = dict(data_file['Acquisition'].attrs)\n",
    "    data_file.close()\n",
    "\n",
    "    # Filtrar los datos\n",
    "    fs        = attrs['MaximumFrequency'] * 2\n",
    "    b, a      = butter(2, (low_cut1, hi_cut1), 'bp', fs=fs)\n",
    "    data_filt = filtfilt(b, a, this_data, axis=0)\n",
    "    x_max     = this_data.shape[1] * attrs['SpatialSamplingInterval']/1000 #km\n",
    "    \n",
    "\n",
    "    # Convertir tiempo a segundos desde el inicio\n",
    "    ts = (this_time - this_time[0]).astype(np.float64) * 1e-6  # Convertir microsegundos a segundos\n",
    "    \n",
    "    # Convertir zoom_start_str y zoom_end_str a segundos desde el inicio\n",
    "    zoom_start_time = mdates.datestr2num(zoom_start_str)\n",
    "    zoom_end_time = mdates.datestr2num(zoom_end_str)\n",
    "\n",
    "    # Convertir tiempos de zoom a segundos desde el inicio\n",
    "    zoom_start = (zoom_start_time - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "    zoom_end = (zoom_end_time - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "    \n",
    "    # Asegurar que el zoom esté dentro de los límites\n",
    "    zoom_start = max(0, zoom_start)\n",
    "    zoom_end = min(ts[-1], zoom_end)\n",
    "    \n",
    "    # Plotear el subgráfico\n",
    "    ax.imshow(data_filt.T, cmap='seismic', aspect='auto', vmin=-0.1, vmax=0.1,\n",
    "              extent=[ts[0], ts[-1], x_max, 0])\n",
    "    \n",
    "    ax.set_xlim(zoom_start, zoom_end)\n",
    "    ax.set_xlabel(\"Time (sec)\", fontsize=25)\n",
    "    ax.set_ylabel(\"Optical distance (km)\", fontsize=25)\n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    ax.xaxis.set_major_locator(mticker.MultipleLocator(2))\n",
    "    ax.text(0.02, 0.98, label, transform=ax.transAxes, fontsize=18, verticalalignment='top', horizontalalignment='left')\n",
    "\n",
    "\n",
    "    # Insertar los puntos desde el CSV\n",
    "    selected_df = pd.read_csv(picks_path)\n",
    "    selected_df\n",
    "    p_wave_label_added = False\n",
    "    s_wave_label_added = False\n",
    "\n",
    "    for index, row in selected_df.iterrows():\n",
    "        blue_point_time = row['trace_s_arrival']\n",
    "        red_point_time = row['trace_p_arrival']\n",
    "        station_code = int(row['station_code'])*attrs['SpatialSamplingInterval']/1000\n",
    "\n",
    "        if pd.notna(red_point_time):\n",
    "            red_point_seconds = (mdates.date2num([red_point_time])[0] - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "            ax.plot(red_point_seconds, station_code, 'ro', markersize=2, label='P-wave' if not p_wave_label_added else \"\")\n",
    "            p_wave_label_added = True\n",
    "\n",
    "        if pd.notna(blue_point_time):\n",
    "            blue_point_seconds = (mdates.date2num([blue_point_time])[0] - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "            ax.plot(blue_point_seconds, station_code, 'bo', markersize=2, label='S-wave' if not s_wave_label_added else \"\")\n",
    "            s_wave_label_added = True\n",
    "\n",
    "    ax.legend(fontsize=15,loc = 'upper right')\n",
    "\n",
    "def plot_data_lastones(ax, file_path, zoom_start_str, zoom_end_str, picks_path, label):\n",
    "    #\n",
    "    data_file = h5py.File(file_path, 'r')\n",
    "    this_data = np.array(data_file['Acquisition/Raw[0]/RawData'][:, chan_min:chan_max])\n",
    "    this_time = np.array(data_file['Acquisition/Raw[0]/RawDataTime'])\n",
    "    attrs     = dict(data_file['Acquisition'].attrs)\n",
    "    x_max     = this_data.shape[1] * attrs['SpatialSamplingInterval']/1000 # km\n",
    "    data_file.close()\n",
    "    \n",
    "    \n",
    "    # filter\n",
    "    fs = attrs['MaximumFrequency'] * 2\n",
    "    b, a = butter(2, (low_cut1, hi_cut1), 'bp', fs=fs)\n",
    "    data_filt = filtfilt(b, a, this_data, axis=0)\n",
    "\n",
    "    # convert time to seconds from the start\n",
    "    ts = (this_time - this_time[0]).astype(np.float64) * 1e-6  # Convert microseconds to seconds\n",
    "    \n",
    "    # convert zoom_start_str and zoom_end_str to seconds from the start\n",
    "    zoom_start_time = mdates.datestr2num(zoom_start_str)\n",
    "    zoom_end_time = mdates.datestr2num(zoom_end_str)\n",
    "\n",
    "    # Convert zoom times to seconds from the start\n",
    "    \n",
    "    zoom_start = (zoom_start_time - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "    zoom_end = (zoom_end_time - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "    \n",
    "    # Ensure zoom is within limits\n",
    "    zoom_start = max(0, zoom_start)\n",
    "    zoom_end = min(ts[-1], zoom_end)\n",
    "    \n",
    "    # Plot the subplot\n",
    "    ax.imshow(data_filt.T, cmap='seismic', aspect='auto', vmin=-0.1, vmax=0.1,\n",
    "              extent=[ts[0], ts[-1], x_max, 0])\n",
    "    \n",
    "\n",
    "    \n",
    "    ax.set_xlim(zoom_start, zoom_end)\n",
    "    ax.set_xlabel(\"Time (sec)\", fontsize=25)\n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.tick_params(axis='y', labelsize=0)\n",
    "    ax.xaxis.set_major_locator(mticker.MultipleLocator(2))\n",
    "    ax.text(0.02, 0.98, label, transform=ax.transAxes, fontsize=18, verticalalignment='top', horizontalalignment='left')\n",
    "\n",
    "\n",
    "    selected_df = pd.read_csv(picks_path)\n",
    "    p_wave_label_added = False\n",
    "    s_wave_label_added = False\n",
    "\n",
    "    for index, row in selected_df.iterrows():\n",
    "        blue_point_time = row['trace_s_arrival']\n",
    "        red_point_time = row['trace_p_arrival']\n",
    "        station_code = int(row['station_code'])*attrs['SpatialSamplingInterval']/1000\n",
    "        \n",
    "        #print(station_code)\n",
    "\n",
    "        if pd.notna(red_point_time):\n",
    "            red_point_seconds = (mdates.date2num([red_point_time])[0] - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "            ax.plot(red_point_seconds, station_code, 'ro', markersize=2, label='P-wave' if not p_wave_label_added else \"\")\n",
    "            p_wave_label_added = True\n",
    "\n",
    "        if pd.notna(blue_point_time):\n",
    "            blue_point_seconds = (mdates.date2num([blue_point_time])[0] - mdates.date2num(this_time[0])) * 24 * 3600\n",
    "            ax.plot(blue_point_seconds, station_code, 'bo', markersize=2, label='S-wave' if not s_wave_label_added else \"\")\n",
    "            s_wave_label_added = True\n",
    "\n",
    "    ax.legend(fontsize=15,loc = 'upper right')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper figures \n",
    "fig, axs = plt.subplots(1, 4, figsize=(18, 7))\n",
    "\n",
    "# List of files with their respective zoom start and end times, and picks file paths\n",
    "files = [\n",
    "    ('/1-fnp/petasaur/p-jbod1/rainier/2023/08/27/decimator_2023-08-27_10.10.00_UTC.h5',\n",
    "     '2023-08-27 10:10:22', '2023-08-27 10:10:32', 'pickerbyhour_2023-08-27_10/file_2023-08-27_10-10.csv'),\n",
    "    \n",
    "    ('/1-fnp/petasaur/p-jbod1/rainier/2023/08/27/decimator_2023-08-27_10.11.00_UTC.h5',\n",
    "     '2023-08-27 10:11:00', '2023-08-27 10:11:10', 'pickerbyhour_2023-08-27_10/file_2023-08-27_10-11.csv'),\n",
    "    \n",
    "    ('/1-fnp/petasaur/p-jbod1/rainier/2023/08/27/decimator_2023-08-27_10.33.00_UTC.h5',\n",
    "     '2023-08-27 10:33:45', '2023-08-27 10:34:55', 'pickerbyhour_2023-08-27_10/file_2023-08-27_10-33.csv'),\n",
    "    \n",
    "    ('/1-fnp/petasaur/p-jbod1/rainier/2023/08/27/decimator_2023-08-27_10.57.00_UTC.h5',\n",
    "     '2023-08-27 10:57:36', '2023-08-27 10:57:46', 'pickerbyhour_2023-08-27_10/file_2023-08-27_10-57.csv')\n",
    "]\n",
    "\n",
    "# **Iteration a, b, c, d**\n",
    "for i, (file_path, start, end, picks) in enumerate(files):\n",
    "    label = f\"{chr(ord('a') + i)})\"  # Generar etiqueta dinámica: 'a)', 'b)', 'c)', 'd)'\n",
    "    if i == 0:\n",
    "        plot_data_first(axs[i], file_path, start, end, picks, label)\n",
    "    else:\n",
    "        plot_data_lastones(axs[i], file_path, start, end, picks, label)\n",
    "\n",
    "#\n",
    "plt.tight_layout()\n",
    "plt.savefig('phasepick-plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Obspy+LibComtCat)",
   "language": "python",
   "name": "python-obspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
